{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Background\n",
    "Banks earn major revenue from lending loans, but this activity also brings risk as borrowers might default on their loan. To mitigate this, banks are leveraging Artificial Intelligence (AI) and Machine Learning (ML) to help predict how likely a borrower is to repay a loan. While this approach can lead to higher returns, banks must be careful when utilizing AI & ML for credit approval decisions due to the presence of bias in training data.\n",
    "\n",
    "For this project, I looked at the impact of using biased data that could be used to train algorithms associated with learning from credit-based data sets. I took a loan approval dataset and used industry-defined fairness metrics to assess whether bias was present in the data. I then applied bias-mitigating techniques to produce a dataset without bias.\n",
    "\n",
    "Identifying and mitigating bias within training data is necessary for AI applications for several reasons:\n",
    "\n",
    "1. Fairness: Today, AI applications are increasingly being used to make decisions that have a significant impact on people's lives, such as hiring, loan approvals, and parole decisions. If these applications are biased, they can unfairly discriminate against certain groups of people, leading to unjust outcomes.\n",
    "\n",
    "2. Accuracy: Inaccuracies in the predictions or recommendations generated by algorithm is another potential consequence of bias in AI applications. For example, if an AI algorithm is trained on biased data, it may make inaccurate predictions for individuals who don't fit the majority pattern in the data.\n",
    "\n",
    "3. Ethics: Discrimination and marginalization of underrepresented groups can perpetuate if AI applications contain bias. Mitigating bias is therefore an ethical imperative, as it helps ensure that AI is used in ways that are consistent with social and moral values.\n",
    "\n",
    "4. Trust: Bias can erode trust in AI applications, as users may be wary of using an algorithm that produces biased outcomes. By mitigating bias, AI developers can build trust in their applications and encourage greater adoption and usage.\n",
    "\n",
    "Overall, taking steps to mitigate bias is necessary to ensure that AI applications are fair, accurate, ethical, and trustworthy. This is essential for building AI systems that benefit society as a whole, rather than perpetuating existing inequalities and injustices."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: Selecting a Dataset\n",
    "I selected a Loan Default Dataset from Kaggle uploaded by M YASSER H in 2022. This dataset concerns credit card application decisions, and is interesting for AI & ML applications because there's a good mix of attributes -- continuous, nominal with a narrow range of values, and nominal with a wide range of values -- and it is subject to strong multi-collinearity & empty values. Of particular interest to me was the fact that this dataset also contained several protected class variables. For this project, I examined whether bias was present when looking at outcomes for 'rate_of_interest' and 'status' associated with different values for 'age' and 'gender'.\n",
    "\n",
    "The 'loan_data.csv' used in this project contains a random sample of 10% of the original Loan Default Dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.impute import KNNImputer\n",
    "from aif360.algorithms.preprocessing import DisparateImpactRemover\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "\n",
    "\n",
    "df = pd.read_csv(r\"Data\\loan_data.csv\")\n",
    "pd.options.mode.chained_assignment = None"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I first cleaned up the data set. This made it easier to work with as I moved through the bias-identification analysis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14770 entries, 0 to 14769\n",
      "Data columns (total 33 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   id                         14770 non-null  object \n",
      " 1   loan_limit                 14452 non-null  object \n",
      " 2   gender                     14770 non-null  object \n",
      " 3   approv_in_adv              14694 non-null  object \n",
      " 4   loan_type                  14770 non-null  object \n",
      " 5   loan_purpose               14755 non-null  object \n",
      " 6   credit_worthiness          14770 non-null  object \n",
      " 7   open_credit                14770 non-null  object \n",
      " 8   business_or_commercial     14770 non-null  object \n",
      " 9   loan_amount                14770 non-null  int64  \n",
      " 10  rate_of_interest           11202 non-null  float64\n",
      " 11  interest_rate_spread       11173 non-null  float64\n",
      " 12  upfront_charges            10891 non-null  float64\n",
      " 13  term                       14764 non-null  float64\n",
      " 14  neg_ammortization          14758 non-null  object \n",
      " 15  interest_only              14770 non-null  object \n",
      " 16  lump_sum_payment           14770 non-null  object \n",
      " 17  property_value             13258 non-null  float64\n",
      " 18  construction_type          14770 non-null  object \n",
      " 19  occupancy_type             14770 non-null  object \n",
      " 20  secured_by                 14770 non-null  object \n",
      " 21  total_units                14770 non-null  object \n",
      " 22  income                     13879 non-null  float64\n",
      " 23  credit_type                14770 non-null  object \n",
      " 24  credit_score               14770 non-null  int64  \n",
      " 25  co-applicant_credit_type   14770 non-null  object \n",
      " 26  age                        14741 non-null  object \n",
      " 27  submission_of_application  14741 non-null  object \n",
      " 28  ltv                        13258 non-null  float64\n",
      " 29  region                     14770 non-null  object \n",
      " 30  security_type              14770 non-null  object \n",
      " 31  status                     14770 non-null  int64  \n",
      " 32  dtir1                      12378 non-null  float64\n",
      "dtypes: float64(8), int64(3), object(22)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# Convert all column names to lowercase\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "# Make ID string type\n",
    "df['id'] = df['id'].astype(str)\n",
    "\n",
    "# Drop year since all entries are 2019\n",
    "df.drop(columns=['year'], inplace=True)\n",
    "\n",
    "df.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, I checked for null values and properly handled those before going any further."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                              0\n",
      "loan_limit                    318\n",
      "gender                          0\n",
      "approv_in_adv                  76\n",
      "loan_type                       0\n",
      "loan_purpose                   15\n",
      "credit_worthiness               0\n",
      "open_credit                     0\n",
      "business_or_commercial          0\n",
      "loan_amount                     0\n",
      "rate_of_interest             3568\n",
      "interest_rate_spread         3597\n",
      "upfront_charges              3879\n",
      "term                            6\n",
      "neg_ammortization              12\n",
      "interest_only                   0\n",
      "lump_sum_payment                0\n",
      "property_value               1512\n",
      "construction_type               0\n",
      "occupancy_type                  0\n",
      "secured_by                      0\n",
      "total_units                     0\n",
      "income                        891\n",
      "credit_type                     0\n",
      "credit_score                    0\n",
      "co-applicant_credit_type        0\n",
      "age                            29\n",
      "submission_of_application      29\n",
      "ltv                          1512\n",
      "region                          0\n",
      "security_type                   0\n",
      "status                          0\n",
      "dtir1                        2392\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in the DataFrame\n",
    "print(df.isnull().sum())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are quite a few columns with null values. I will go down the list and start handling them properly."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# Getting dataframes by datatype\n",
    "data_types = pd.DataFrame(df.dtypes).reset_index()\n",
    "\n",
    "categorical_vars = []\n",
    "numerical_vars = []\n",
    "for i, l in zip(data_types['index'], data_types[0]):\n",
    "    if l == 'object':\n",
    "        categorical_vars.append(i)\n",
    "    else:\n",
    "        numerical_vars.append(i)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "I started with the numerical data first. One of the best methods for replacing Null values for numerical data is using a KNNImputer from sklearn. I could have replaced the Null values using the column median or mean, but KNNImputer uses the n-number of neighbor-columns that have values (at that index) to estimate the null value of a certain entry. In this example, I used 2 neighbors. This method produces a more \"realistic\" substitution of values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# Numeric Dataframe\n",
    "df_num = df[numerical_vars]\n",
    "\n",
    "# knn\n",
    "knn = KNNImputer(n_neighbors = 2)\n",
    "knn.fit(df_num)\n",
    "X = knn.fit_transform(df_num)\n",
    "\n",
    "# Check for any nas\n",
    "df_num = pd.DataFrame(X, columns=numerical_vars)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next I handled the categorical data with Null values. Here, I took the modal value for each column. Once all Null values were imputed, I concatenated the numeric and categorical dataframes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loan_amount                  0\n",
      "rate_of_interest             0\n",
      "interest_rate_spread         0\n",
      "upfront_charges              0\n",
      "term                         0\n",
      "property_value               0\n",
      "income                       0\n",
      "credit_score                 0\n",
      "ltv                          0\n",
      "status                       0\n",
      "dtir1                        0\n",
      "id                           0\n",
      "loan_limit                   0\n",
      "gender                       0\n",
      "approv_in_adv                0\n",
      "loan_type                    0\n",
      "loan_purpose                 0\n",
      "credit_worthiness            0\n",
      "open_credit                  0\n",
      "business_or_commercial       0\n",
      "neg_ammortization            0\n",
      "interest_only                0\n",
      "lump_sum_payment             0\n",
      "construction_type            0\n",
      "occupancy_type               0\n",
      "secured_by                   0\n",
      "total_units                  0\n",
      "credit_type                  0\n",
      "co-applicant_credit_type     0\n",
      "age                          0\n",
      "submission_of_application    0\n",
      "region                       0\n",
      "security_type                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Categorical Dataframe\n",
    "df_cat = df[categorical_vars]\n",
    "\n",
    "for i in categorical_vars:\n",
    "    mode = df[i].mode()\n",
    "    mode = mode[0]\n",
    "    df_cat[i].fillna(value=mode, inplace=True)\n",
    "\n",
    "# Combining dataframes\n",
    "df_new = pd.concat([df_num, df_cat], axis=1, join='inner')\n",
    "print(df_new.isna().sum())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Segmenting Based on Protected Class and Dependent Variables\n",
    "\n",
    "### 2.1\n",
    "For this exercise, I identified  members associated with my protected class variables, 'age' and 'gender', and grouped them together into a subset of membership categories.\n",
    "\n",
    "First, I checked the unique values for my variables to determine if they needed to be grouped into subcategories."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values and frequency count for age:\n",
      "25-34    1901\n",
      "35-44    3298\n",
      "45-54    3496\n",
      "55-64    3184\n",
      "65-74    2095\n",
      "<25       142\n",
      ">74       654\n",
      "Name: age, dtype: int64\n",
      "\n",
      "\n",
      "0    The mode for age is 45-54\n",
      "Name: age, dtype: object\n",
      "\n",
      "\n",
      "Unique values and frequency count for gender:\n",
      "Female               2755\n",
      "Joint                4043\n",
      "Male                 4159\n",
      "Sex Not Available    3813\n",
      "Name: gender, dtype: int64\n",
      "\n",
      "\n",
      "0    The mode for gender is Male\n",
      "Name: gender, dtype: object\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select two specific columns\n",
    "cols = ['age', 'gender']\n",
    "\n",
    "# print the unique values for the selected columns in table format\n",
    "for col in cols:\n",
    "    print(f'Unique values and frequency count for {col}:')\n",
    "    print(df_new[col].value_counts().sort_index())\n",
    "    print('\\n')\n",
    "    print(f'The mode for {col} is ' + df_new[col].mode())\n",
    "    print('\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Based on the values, I decided to group 'age' because it had 7 unique values whereas 'gender' had only four unique values.\n",
    "\n",
    "I created a new field 'derived_age' to discretize the values of 'age'. Since the values for 'age' spanned from <25 to >74, I used the discrete categories \"Young\", \"Middle-Aged\", and \"Old\", to represent the 'age' values from 0 to 34, 35 to 54, and 55+, respectively.\n",
    "\n",
    "After creating the new field, I generated a table documenting the relationship between members and membership categories for the 'age' and 'derived_age' protected class variables. Since I left 'gender' unchanged, I didn't generate a table for that protect class variable."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age          25-34  35-44  45-54  55-64  65-74  <25  >74\n",
      "derived_age                                             \n",
      "Young         1901      0      0      0      0  142    0\n",
      "Middle-Aged      0   3298   3496      0      0    0    0\n",
      "Old              0      0      0   3184   2095    0  654\n"
     ]
    }
   ],
   "source": [
    "# Define the conditions and corresponding values for the derived_age column\n",
    "conditions = [(df_new['age'] == '<25') | (df_new['age'] == '25-34'),\n",
    "              (df_new['age'] == '35-44') | (df_new['age'] == '45-54'),\n",
    "              (df_new['age'] == '55-64') | (df_new['age'] == '65-74') | (df_new['age'] == '>74')]\n",
    "\n",
    "values = ['Young', 'Middle-Aged', 'Old']\n",
    "\n",
    "# Apply the conditions and values to the derived_age column using numpy.select()\n",
    "df_new['derived_age'] = np.select(conditions, values)\n",
    "\n",
    "# Create a cross-tabulation table of derived_age and age\n",
    "ct = pd.crosstab(df_new['derived_age'], df_new['age'])\n",
    "ct = ct.loc[values]\n",
    "\n",
    "# Print the table\n",
    "print(ct)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2\n",
    "Next, I discretized the values associated with my dependent variables into categories/numerical values, as appropriate. Again, I first checked the unique values for 'rate_of_interest' and 'status' to determine if they needed to be grouped into subcategories."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values and frequency count for rate_of_interest:\n",
      "2.2500     1\n",
      "2.4750     1\n",
      "2.5750     1\n",
      "2.7500    71\n",
      "2.8125     6\n",
      "          ..\n",
      "6.1250     1\n",
      "6.2400    23\n",
      "6.2500     1\n",
      "6.5000     1\n",
      "7.5000     1\n",
      "Name: rate_of_interest, Length: 208, dtype: int64\n",
      "\n",
      "\n",
      "Unique values and frequency count for status:\n",
      "0.0    11173\n",
      "1.0     3597\n",
      "Name: status, dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select two specific columns\n",
    "cols = ['rate_of_interest', 'status']\n",
    "\n",
    "# print the unique values for the selected columns in table format\n",
    "for col in cols:\n",
    "    print(f'Unique values and frequency count for {col}:')\n",
    "    print(df_new[col].value_counts().sort_index())\n",
    "    print('\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Based on the values, I grouped 'rate_of_interest', but not 'status' - 'rate_of_interest' had dozens of unique values whereas 'status' had only two unique values.\n",
    "\n",
    "First, I created a new field 'derived_interest' to discretize the values of 'rate_of_interest'. Since the values for 'rate_of_interest' span from 0 to 8, I used the discrete categories \"Low\", \"Med-Low\", \"Med-High\", and \"High\" to represent 'rate_of_interest' values from 0 to 3, 3 to 4, 4 to 5, and 5+, respectively.\n",
    "\n",
    "After creating the new field, I generated a table documenting the frequency of and relationship between members and membership categories for the 'rate_of_interest' and 'derived_interest' dependent variables. Since I left 'status' unchanged, I didn't generate a table for that dependent variable."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "derived_interest  High  Low  Med-High  Med-Low\n",
      "rate_of_interest                              \n",
      "2.2500               0    1         0        0\n",
      "2.4750               0    1         0        0\n",
      "2.5750               0    1         0        0\n",
      "2.7500               0   71         0        0\n",
      "2.8125               0    6         0        0\n",
      "...                ...  ...       ...      ...\n",
      "6.1250               1    0         0        0\n",
      "6.2400              23    0         0        0\n",
      "6.2500               1    0         0        0\n",
      "6.5000               1    0         0        0\n",
      "7.5000               1    0         0        0\n",
      "\n",
      "[208 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define the conditions and corresponding values for the derived_interest column\n",
    "conditions = [df_new['rate_of_interest'] < 3,\n",
    "    (df_new['rate_of_interest'] >= 3) & (df_new['rate_of_interest'] < 4),\n",
    "    (df_new['rate_of_interest'] >= 4) & (df_new['rate_of_interest'] < 5),\n",
    "    (df_new['rate_of_interest'] >= 5)]\n",
    "\n",
    "values = ['Low', 'Med-Low', 'Med-High', 'High']\n",
    "\n",
    "# Apply the conditions and values to the derived_interest column using numpy.select()\n",
    "df_new['derived_interest'] = np.select(conditions, values)\n",
    "\n",
    "# Create a cross-tabulation table of derived_interest and rate_of_interest\n",
    "ct = pd.crosstab(df_new['derived_interest'], df_new['rate_of_interest'])\n",
    "ct = ct.T\n",
    "print(ct)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3\n",
    " I then generated frequency tables for the protected class and dependent variables to make sure there were sufficient values for analysis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values and frequency count for status:\n",
      "0.0    11173\n",
      "1.0     3597\n",
      "Name: status, dtype: int64\n",
      "\n",
      "\n",
      "Unique values and frequency count for derived_interest:\n",
      "Low          366\n",
      "Med-Low     8132\n",
      "Med-High    5872\n",
      "High         400\n",
      "Name: derived_interest, dtype: int64\n",
      "\n",
      "\n",
      "Unique values and frequency count for gender:\n",
      "Male                 4159\n",
      "Female               2755\n",
      "Joint                4043\n",
      "Sex Not Available    3813\n",
      "Name: gender, dtype: int64\n",
      "\n",
      "\n",
      "Unique values and frequency count for derived_age:\n",
      "Young          2043\n",
      "Middle-Aged    6794\n",
      "Old            5933\n",
      "Name: derived_age, dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select specific columns\n",
    "cols = ['status', 'derived_interest', 'gender', 'derived_age']\n",
    "\n",
    "# define the desired sort order\n",
    "interest_sort_order = ['Low', 'Med-Low', 'Med-High', 'High']\n",
    "age_sort_order = ['Young', 'Middle-Aged', 'Old']\n",
    "gender_sort_order = ['Male', 'Female', 'Joint', 'Sex Not Available']\n",
    "\n",
    "# print the unique values for the selected columns in table format\n",
    "for col in cols:\n",
    "    print(f'Unique values and frequency count for {col}:')\n",
    "    if col == 'derived_interest':\n",
    "        print(df_new[col].value_counts().reindex(interest_sort_order))\n",
    "    elif col == 'derived_age':\n",
    "        print(df_new[col].value_counts().reindex(age_sort_order))\n",
    "    elif col == 'gender':\n",
    "        print(df_new[col].value_counts().reindex(gender_sort_order))\n",
    "    else:\n",
    "        print(df_new[col].value_counts().sort_index())\n",
    "    print('\\n')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.4\n",
    "Next, I created histograms for each protected class variable that graphs the relative proportions of values of its membership categories as a function of the dependent variables. For this exercise I used the discrete transformations for each of the protected and dependent variables.\n",
    "\n",
    "This exercise helped me visualize the data and identify areas of potential bias."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mbaer30\\AppData\\Local\\Temp\\ipykernel_16924\\517262856.py:14: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "C:\\Users\\mbaer30\\AppData\\Local\\Temp\\ipykernel_16924\\517262856.py:25: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "C:\\Users\\mbaer30\\AppData\\Local\\Temp\\ipykernel_16924\\517262856.py:14: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "C:\\Users\\mbaer30\\AppData\\Local\\Temp\\ipykernel_16924\\517262856.py:25: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Histograms\n",
    "protected = ['gender', 'derived_age']\n",
    "\n",
    "for i in protected:\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    sns.set_theme(style='darkgrid')\n",
    "\n",
    "    # Calculate the proportions relative to 'derived_interest'\n",
    "    interest_proportions = df_new.groupby(i)['derived_interest'].value_counts(normalize=True).reset_index(name='proportion')\n",
    "\n",
    "    ax = sns.barplot(data=interest_proportions, x=i, y='proportion', hue=\"derived_interest\")\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f\"{p.get_height():.2f}\", (p.get_x() + p.get_width() / 2, p.get_height()), ha='center', va='bottom')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    sns.set_theme(style='darkgrid')\n",
    "\n",
    "    # Calculate the proportions relative to 'statue'\n",
    "    status_proportions = df_new.groupby(i)['status'].value_counts(normalize=True).reset_index(name='proportion')\n",
    "\n",
    "    ax = sns.barplot(data=status_proportions, x=i, y='proportion', hue=\"status\")\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f\"{p.get_height():.2f}\", (p.get_x() + p.get_width() / 2, p.get_height()), ha='center', va='bottom')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the histograms, it was hard to tell whether any bias was present. To be sure, I used industry-recognized bias metrics to further examine the data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3: Applying Fairness Metrics\n",
    "\n",
    "### 3.1\n",
    "For this exercise, I started by identifying the privileged/unprivileged groups associated with each of my protected class variables, as well as the favorable/unfavorable outcomes associated with each of my dependent variables:\n",
    "\n",
    "Protected Class Variables:\n",
    "'derived_age':\n",
    "* privileged:   'Young'\n",
    "* unprivileged: 'Middle-Aged', 'Old'\n",
    "\n",
    "'gender':\n",
    "* privileged: 'Male'\n",
    "* unprivileged: 'Female', 'Joint', 'Sex Not Available'\n",
    "\n",
    "### 3.2\n",
    "For each protected class variable, I'll now select two fairness metrics and compute the fairness metrics associated with my privileged/unprivileged groups as a function of each of my two dependent variables.\n",
    "To do this, I'll first assign a reasonable threshold to each dependent variable in order to generate a baseline for comparison using the fairness metrics.\n",
    "\n",
    "Dependent Variables:\n",
    "'derived_interest':\n",
    "* favorable:   'Low'\n",
    "* unfavorable: 'Med-Low', 'Med-High', 'High'\n",
    "\n",
    "'status':\n",
    "* favorable:   0\n",
    "* unfavorable: 1\n",
    "\n",
    "The two fairness metrics I used were Statistical Parity Difference and Disparate Impact."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To compute the Statistical Parity Difference, I'll compute the difference of the rate of favorable outcomes received by the unprivileged group to the privileged group. The ideal value of this metric is 0. Fairness for this metric is between -0.1 and 0.1."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPD for 'derived_age' and 'derived_interest': -0.010981172372928047\n",
      "SPD for 'derived_age' and 'status': 0.03355486650456718\n",
      "SPD for 'gender' and 'derived_interest': -0.0057264521984822375\n",
      "SPD for 'gender' and 'status': -0.0761530384389072\n"
     ]
    }
   ],
   "source": [
    "# Statistical Parity Difference function\n",
    "def spd(data, protected_var, dependent_var, privileged_group, unprivileged_groups, favorable_outcome):\n",
    "\n",
    "\n",
    "    privileged_rate = data[(data[protected_var] == privileged_group) & (data[dependent_var] == favorable_outcome)][dependent_var].count() / data[data[protected_var] == privileged_group][dependent_var].count()\n",
    "\n",
    "    unprivileged_rate = data[(data[protected_var].isin(unprivileged_groups)) & (data[dependent_var] == favorable_outcome)][dependent_var].count() / data[data[protected_var].isin(unprivileged_groups)][dependent_var].count()\n",
    "\n",
    "    return unprivileged_rate - privileged_rate\n",
    "\n",
    "# Assigning baseline thresholds for each dependent variable\n",
    "interest_threshold = 'Low'\n",
    "status_threshold = 0\n",
    "\n",
    "# Computing SPD for derived_age\n",
    "derived_age_privileged = 'Old'\n",
    "derived_age_unprivileged = ['Middle-Aged', 'Young']\n",
    "\n",
    "spd_derived_age_interest = spd(df_new, 'derived_age', 'derived_interest', derived_age_privileged, derived_age_unprivileged, interest_threshold)\n",
    "\n",
    "spd_derived_age_status = spd(df_new, 'derived_age', 'status', derived_age_privileged, derived_age_unprivileged, status_threshold)\n",
    "\n",
    "# Computing SPD for gender\n",
    "gender_privileged = 'Joint'\n",
    "gender_unprivileged = ['Female', 'Male', 'Sex Not Available']\n",
    "\n",
    "spd_gender_interest = spd(df_new, 'gender', 'derived_interest', gender_privileged, gender_unprivileged, interest_threshold)\n",
    "\n",
    "spd_gender_status = spd(df_new, 'gender', 'status', gender_privileged, gender_unprivileged, status_threshold)\n",
    "\n",
    "print(\"SPD for 'derived_age' and 'derived_interest':\", spd_derived_age_interest)\n",
    "print(\"SPD for 'derived_age' and 'status':\", spd_derived_age_status)\n",
    "print(\"SPD for 'gender' and 'derived_interest':\", spd_gender_interest)\n",
    "print(\"SPD for 'gender' and 'status':\", spd_gender_status)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Statistical Parity Difference values looked good. Next, I looked at the Disparate Impact values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To compute the Disparate Impact, I'll compute the ratio of rate of favorable outcome for the unprivileged group to that of the privileged group. The ideal value of this metric is 1.0. A value < 1 implies higher benefit for the privileged group and a value >1 implies a higher benefit for the unprivileged group. Fairness for this metric is between 0.8 and 1.25."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DI for 'derived_age' and 'derived_interest': 0.6497242167280532\n",
      "DI for 'derived_age' and 'status': 1.045566725331105\n",
      "DI for 'gender' and 'derived_interest': 0.8021192629191137\n",
      "DI for 'gender' and 'status': 0.9061892948176411\n"
     ]
    }
   ],
   "source": [
    "# Disparate Impact function\n",
    "def disp_impact(data, protected_var, dependent_var, privileged_group, unprivileged_groups, favorable_outcome):\n",
    "\n",
    "\n",
    "    privileged_rate = data[(data[protected_var] == privileged_group) & (data[dependent_var] == favorable_outcome)][dependent_var].count() / data[data[protected_var] == privileged_group][dependent_var].count()\n",
    "\n",
    "    unprivileged_rate = data[(data[protected_var].isin(unprivileged_groups)) & (data[dependent_var] == favorable_outcome)][dependent_var].count() / data[data[protected_var].isin(unprivileged_groups)][dependent_var].count()\n",
    "\n",
    "    return unprivileged_rate / privileged_rate\n",
    "\n",
    "di_derived_age_interest = disp_impact(df_new, 'derived_age', 'derived_interest', derived_age_privileged, derived_age_unprivileged, interest_threshold)\n",
    "\n",
    "di_derived_age_status = disp_impact(df_new, 'derived_age', 'status', derived_age_privileged, derived_age_unprivileged, status_threshold)\n",
    "\n",
    "di_gender_interest = disp_impact(df_new, 'gender', 'derived_interest', gender_privileged, gender_unprivileged, interest_threshold)\n",
    "\n",
    "di_gender_status = disp_impact(df_new, 'gender', 'status', gender_privileged, gender_unprivileged, status_threshold)\n",
    "\n",
    "print(\"DI for 'derived_age' and 'derived_interest':\", di_derived_age_interest)\n",
    "print(\"DI for 'derived_age' and 'status':\", di_derived_age_status)\n",
    "print(\"DI for 'gender' and 'derived_interest':\", di_gender_interest)\n",
    "print(\"DI for 'gender' and 'status':\", di_gender_status)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The values for Disparate Impact indicated some bias was present."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3\n",
    "Next, I selected a pre-processing bias mitigation algorithm to transform the dataset as a function of 'derived_interest'. I used the 'derived_interest' and 'gender' combination for this exercise because the Disparate Impact values for this combination tended to be closer to the fairness range edges, or outside the fairness range, compared to other combinations.\n",
    "\n",
    "aif360 has a DisparateImpactRemover that I used, but before I applied the bias-mitigating algorithm, I needed to ensure my data frame contained only numeric values. To do this, I used Pandas' factorize() method."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14770 entries, 0 to 14769\n",
      "Data columns (total 35 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   loan_amount                14770 non-null  float64\n",
      " 1   rate_of_interest           14770 non-null  float64\n",
      " 2   interest_rate_spread       14770 non-null  float64\n",
      " 3   upfront_charges            14770 non-null  float64\n",
      " 4   term                       14770 non-null  float64\n",
      " 5   property_value             14770 non-null  float64\n",
      " 6   income                     14770 non-null  float64\n",
      " 7   credit_score               14770 non-null  float64\n",
      " 8   ltv                        14770 non-null  float64\n",
      " 9   status                     14770 non-null  float64\n",
      " 10  dtir1                      14770 non-null  float64\n",
      " 11  id                         14770 non-null  int64  \n",
      " 12  loan_limit                 14770 non-null  int64  \n",
      " 13  gender                     14770 non-null  int64  \n",
      " 14  approv_in_adv              14770 non-null  int64  \n",
      " 15  loan_type                  14770 non-null  int64  \n",
      " 16  loan_purpose               14770 non-null  int64  \n",
      " 17  credit_worthiness          14770 non-null  int64  \n",
      " 18  open_credit                14770 non-null  int64  \n",
      " 19  business_or_commercial     14770 non-null  int64  \n",
      " 20  neg_ammortization          14770 non-null  int64  \n",
      " 21  interest_only              14770 non-null  int64  \n",
      " 22  lump_sum_payment           14770 non-null  int64  \n",
      " 23  construction_type          14770 non-null  int64  \n",
      " 24  occupancy_type             14770 non-null  int64  \n",
      " 25  secured_by                 14770 non-null  int64  \n",
      " 26  total_units                14770 non-null  int64  \n",
      " 27  credit_type                14770 non-null  int64  \n",
      " 28  co-applicant_credit_type   14770 non-null  int64  \n",
      " 29  age                        14770 non-null  int64  \n",
      " 30  submission_of_application  14770 non-null  int64  \n",
      " 31  region                     14770 non-null  int64  \n",
      " 32  security_type              14770 non-null  int64  \n",
      " 33  derived_age                14770 non-null  int64  \n",
      " 34  derived_interest           14770 non-null  int64  \n",
      "dtypes: float64(11), int64(24)\n",
      "memory usage: 3.9 MB\n"
     ]
    }
   ],
   "source": [
    "# New datatypes to include our derived attributes\n",
    "dtypes = pd.DataFrame(df_new.dtypes).reset_index()\n",
    "cat_vars = []\n",
    "num_vars = []\n",
    "for i, l in zip(dtypes['index'], dtypes[0]):\n",
    "    if l == 'object':\n",
    "        cat_vars.append(i)\n",
    "    else:\n",
    "        num_vars.append(i)\n",
    "\n",
    "# Binary variables\n",
    "binary_vars = ['security_type', 'submission_of_application', 'co-applicant_credit_type', 'secured_by',\n",
    "               'lump_sum_payment', 'interest_only', 'neg_ammortization', 'construction_type', 'business_or_commercial',\n",
    "               'open_credit', 'credit_worthiness', 'approv_in_adv', 'loan_limit', 'status']\n",
    "\n",
    "# Using pandas' factorize(), I transform the categorical data\n",
    "df_new[cat_vars] = df_new[cat_vars].apply(lambda x: pd.factorize(x)[0])\n",
    "df_numeric = df_new\n",
    "df_numeric.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mbaer30\\AppData\\Local\\Temp\\ipykernel_16924\\3414452477.py:14: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "C:\\Users\\mbaer30\\AppData\\Local\\Temp\\ipykernel_16924\\3414452477.py:26: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "C:\\Users\\mbaer30\\AppData\\Local\\Temp\\ipykernel_16924\\3414452477.py:14: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "C:\\Users\\mbaer30\\AppData\\Local\\Temp\\ipykernel_16924\\3414452477.py:26: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# Histograms\n",
    "protected = ['gender', 'derived_age']\n",
    "\n",
    "for i in protected:\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    sns.set_theme(style='darkgrid')\n",
    "\n",
    "    # Calculate the proportions relative to 'derived_age'\n",
    "    interest_proportions = df_numeric.groupby(i)['derived_interest'].value_counts(normalize=True).reset_index(name='proportion')\n",
    "\n",
    "    ax = sns.barplot(data=interest_proportions, x=i, y='proportion', hue=\"derived_interest\")\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f\"{p.get_height():.2f}\", (p.get_x() + p.get_width() / 2, p.get_height()), ha='center', va='bottom')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    sns.set_theme(style='darkgrid')\n",
    "\n",
    "    # Calculate the proportions relative to 'derived_age'\n",
    "    interest_proportions = df_numeric.groupby(i)['status'].value_counts(normalize=True).reset_index(name='proportion')\n",
    "\n",
    "    ax = sns.barplot(data=interest_proportions, x=i, y='proportion', hue=\"status\")\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f\"{p.get_height():.2f}\", (p.get_x() + p.get_width() / 2, p.get_height()), ha='center', va='bottom')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that all the values are numeric, I can move to pre-processing."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "# Convert multi-class labels into binary labels; 'derived_interest' of 3 = 'Low'\n",
    "df_binary = df_numeric.copy()\n",
    "df_binary['derived_interest'] = df_binary['derived_interest'].apply(lambda x: 1 if x == 3 else 0)\n",
    "\n",
    "binary_label_dataset = BinaryLabelDataset(\n",
    "    favorable_label = 1,\n",
    "    unfavorable_label = 0,\n",
    "    df = df_binary,\n",
    "    label_names = ['derived_interest'],\n",
    "    protected_attribute_names = ['gender'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 14770 entries, 0 to 14769\n",
      "Data columns (total 35 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   loan_amount                14770 non-null  float64\n",
      " 1   rate_of_interest           14770 non-null  float64\n",
      " 2   interest_rate_spread       14770 non-null  float64\n",
      " 3   upfront_charges            14770 non-null  float64\n",
      " 4   term                       14770 non-null  float64\n",
      " 5   property_value             14770 non-null  float64\n",
      " 6   income                     14770 non-null  float64\n",
      " 7   credit_score               14770 non-null  float64\n",
      " 8   ltv                        14770 non-null  float64\n",
      " 9   status                     14770 non-null  float64\n",
      " 10  dtir1                      14770 non-null  float64\n",
      " 11  id                         14770 non-null  float64\n",
      " 12  loan_limit                 14770 non-null  float64\n",
      " 13  gender                     14770 non-null  float64\n",
      " 14  approv_in_adv              14770 non-null  float64\n",
      " 15  loan_type                  14770 non-null  float64\n",
      " 16  loan_purpose               14770 non-null  float64\n",
      " 17  credit_worthiness          14770 non-null  float64\n",
      " 18  open_credit                14770 non-null  float64\n",
      " 19  business_or_commercial     14770 non-null  float64\n",
      " 20  neg_ammortization          14770 non-null  float64\n",
      " 21  interest_only              14770 non-null  float64\n",
      " 22  lump_sum_payment           14770 non-null  float64\n",
      " 23  construction_type          14770 non-null  float64\n",
      " 24  occupancy_type             14770 non-null  float64\n",
      " 25  secured_by                 14770 non-null  float64\n",
      " 26  total_units                14770 non-null  float64\n",
      " 27  credit_type                14770 non-null  float64\n",
      " 28  co-applicant_credit_type   14770 non-null  float64\n",
      " 29  age                        14770 non-null  float64\n",
      " 30  submission_of_application  14770 non-null  float64\n",
      " 31  region                     14770 non-null  float64\n",
      " 32  security_type              14770 non-null  float64\n",
      " 33  derived_age                14770 non-null  float64\n",
      " 34  derived_interest           14770 non-null  float64\n",
      "dtypes: float64(35)\n",
      "memory usage: 4.1+ MB\n"
     ]
    }
   ],
   "source": [
    "di = DisparateImpactRemover(repair_level = 1.0)\n",
    "dataset_transf_train = di.fit_transform(binary_label_dataset)\n",
    "df_transformed = dataset_transf_train.convert_to_dataframe()[0]\n",
    "df_transformed.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4\n",
    "Now I'll use the two fairness metrics identified earlier and compute the fairness metrics on the transformed dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset\n",
      "SPD for 'derived_age' and 'derived_interest': -0.010981172372928047\n",
      "SPD for 'derived_age' and 'status': 0.03355486650456718\n",
      "SPD for 'gender' and 'derived_interest': -0.0057264521984822375\n",
      "SPD for 'gender' and 'status': -0.0761530384389072\n",
      "DI for 'derived_age' and 'derived_interest': 0.6497242167280532\n",
      "DI for 'derived_age' and 'status': 1.045566725331105\n",
      "DI for 'gender' and 'derived_interest': 0.8021192629191137\n",
      "DI for 'gender' and 'status': 0.9061892948176411\n",
      "\n",
      " Transformed Dataset\n",
      "SPD for 'derived_age' and 'derived_interest': 0.0\n",
      "SPD for 'derived_age' and 'status': 0.03355486650456718\n",
      "SPD for 'gender' and 'derived_interest': 0.0\n",
      "SPD for 'gender' and 'status': -0.0761530384389072\n",
      "DI for 'derived_age' and 'derived_interest': nan\n",
      "DI for 'derived_age' and 'status': 1.045566725331105\n",
      "DI for 'gender' and 'derived_interest': nan\n",
      "DI for 'gender' and 'status': 0.9061892948176411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mbaer30\\AppData\\Local\\Temp\\ipykernel_16924\\2926725050.py:9: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return unprivileged_rate / privileged_rate\n"
     ]
    }
   ],
   "source": [
    "# Assigning baseline thresholds for each dependent variable\n",
    "trans_interest_threshold = 3\n",
    "trans_status_threshold = 0\n",
    "\n",
    "# Computing SPD for transformed derived_age\n",
    "trans_derived_age_privileged = 0\n",
    "trans_derived_age_unprivileged = [1, 2]\n",
    "\n",
    "# Computing SPD for transformed gender\n",
    "trans_gender_privileged = 3\n",
    "trans_gender_unprivileged = [0, 1, 2]\n",
    "\n",
    "trans_spd_derived_age_interest = spd(df_transformed, 'derived_age', 'derived_interest', trans_derived_age_privileged, trans_derived_age_unprivileged, trans_interest_threshold)\n",
    "\n",
    "trans_spd_derived_age_status = spd(df_transformed, 'derived_age', 'status', trans_derived_age_privileged, trans_derived_age_unprivileged, trans_status_threshold)\n",
    "\n",
    "trans_spd_gender_interest = spd(df_transformed, 'gender', 'derived_interest', trans_gender_privileged, trans_gender_unprivileged, trans_interest_threshold)\n",
    "\n",
    "trans_spd_gender_status = spd(df_transformed, 'gender', 'status', trans_gender_privileged, trans_gender_unprivileged, trans_status_threshold)\n",
    "\n",
    "trans_di_derived_age_interest = disp_impact(df_transformed, 'derived_age', 'derived_interest', trans_derived_age_privileged, trans_derived_age_unprivileged, trans_interest_threshold)\n",
    "\n",
    "trans_di_derived_age_status = disp_impact(df_transformed, 'derived_age', 'status', trans_derived_age_privileged, trans_derived_age_unprivileged, trans_status_threshold)\n",
    "\n",
    "trans_di_gender_interest = disp_impact(df_transformed, 'gender', 'derived_interest', trans_gender_privileged, trans_gender_unprivileged, trans_interest_threshold)\n",
    "\n",
    "trans_di_gender_status = disp_impact(df_transformed, 'gender', 'status', trans_gender_privileged, trans_gender_unprivileged, trans_status_threshold)\n",
    "\n",
    "print(\"Original Dataset\")\n",
    "print(\"SPD for 'derived_age' and 'derived_interest':\", spd_derived_age_interest)\n",
    "print(\"SPD for 'derived_age' and 'status':\", spd_derived_age_status)\n",
    "print(\"SPD for 'gender' and 'derived_interest':\", spd_gender_interest)\n",
    "print(\"SPD for 'gender' and 'status':\", spd_gender_status)\n",
    "print(\"DI for 'derived_age' and 'derived_interest':\", di_derived_age_interest)\n",
    "print(\"DI for 'derived_age' and 'status':\", di_derived_age_status)\n",
    "print(\"DI for 'gender' and 'derived_interest':\", di_gender_interest)\n",
    "print(\"DI for 'gender' and 'status':\", di_gender_status)\n",
    "\n",
    "print('\\n','Transformed Dataset')\n",
    "print(\"SPD for 'derived_age' and 'derived_interest':\", trans_spd_derived_age_interest)\n",
    "print(\"SPD for 'derived_age' and 'status':\", trans_spd_derived_age_status)\n",
    "print(\"SPD for 'gender' and 'derived_interest':\", trans_spd_gender_interest)\n",
    "print(\"SPD for 'gender' and 'status':\", trans_spd_gender_status)\n",
    "print(\"DI for 'derived_age' and 'derived_interest':\", trans_di_derived_age_interest)\n",
    "print(\"DI for 'derived_age' and 'status':\", trans_di_derived_age_status)\n",
    "print(\"DI for 'gender' and 'derived_interest':\", trans_di_gender_interest)\n",
    "print(\"DI for 'gender' and 'status':\", trans_di_gender_status)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The transformed dataset mitigated bias for the 'derived_interest' column concerning both 'derived_age' and 'gender' - exactly as anticipated! One step closer to using this dataset for AI training."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
